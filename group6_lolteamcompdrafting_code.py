# -*- coding: utf-8 -*-
"""group6_LoLTeamCompDrafting_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qf0lDcd6zeqVmzHPrsGoDp1sRDXgg6k6
"""

import pandas as pd

# Load the dataset
data_path = 'LeagueTeamDataClean.xlsx'
league_data = pd.read_excel(data_path)

# Display the first few rows to check the data
print(league_data.head())

# Calculate the frequency of each character in each position
character_counts = {col: league_data[col].value_counts() for col in league_data.columns}
for position, counts in character_counts.items():
    print(f"Position: {position}")
    print(counts)
    print("\n")

import matplotlib.pyplot as plt

# Plot the frequency of the top 5 characters in each role
for position, counts in character_counts.items():
    top_characters = counts.head(5)
    plt.figure(figsize=(10, 5))
    top_characters.plot(kind='bar')
    plt.title(f'Top 5 Characters in {position}')
    plt.ylabel('Frequency')
    plt.xlabel('Character')
    plt.show()

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten
from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# Load and prepare the data
league_data = pd.read_excel('LeagueTeamDataClean.xlsx')
label_encoder = LabelEncoder()

# Flatten the DataFrame to a 1D array, fit transform, then reshape back
all_labels = label_encoder.fit_transform(league_data.values.flatten())

# Reshape and split into input features and target feature
X_encoded = all_labels.reshape(league_data.shape)[:, :-1]  # Input features
y_encoded = all_labels.reshape(league_data.shape)[:, -1]   # Target feature

# Since label_encoder was fit on all data, we use its classes_ to get the right input_dim
input_dim = len(label_encoder.classes_)

print(f"Input dimension (input_dim): {input_dim}")

# Neural network model
model = Sequential([
    Embedding(input_dim=input_dim, output_dim=50, input_length=X_encoded.shape[1]),
    Flatten(),
    Dense(100, activation='relu'),
    Dropout(0.5),
    Dense(input_dim, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', f1_m,precision_m, recall_m])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)

# Fit the model
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

model = Sequential([
    Embedding(input_dim=input_dim, output_dim=50, input_length=X_encoded.shape[1]),
    Flatten(),
    Dense(256, activation='relu'),  # Increased from 100 to 256
    Dropout(0.5),
    Dense(128, activation='relu'),  # Additional layer
    Dropout(0.3),  # Adjusted dropout
    Dense(input_dim, activation='softmax')
])

from tensorflow.keras.optimizers import Adam

optimizer = Adam(learning_rate=0.001)  # You can try different learning rates
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.optimizers import SGD

optimizer = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, BatchNormalization
from tensorflow.keras.optimizers import Adam

# Define the model architecture (or make changes to it)
model = Sequential([
    Embedding(input_dim=input_dim, output_dim=50, input_length=X_encoded.shape[1]),
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    BatchNormalization(),  # Example of an added layer
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(input_dim, activation='softmax')
])

# Compile the model with new settings
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Now fit the model
model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Define or redefine your model architecture
model = Sequential([
    Embedding(input_dim=input_dim, output_dim=50, input_length=X_encoded.shape[1]),
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    BatchNormalization(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    BatchNormalization(),
    Dense(input_dim, activation='softmax')
])

# Compile the model with new settings
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy', f1_m,precision_m, recall_m])

# Set up early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Fit the model with early stopping
model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, BatchNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load and prepare your data
league_data = pd.read_excel('LeagueTeamDataClean.xlsx')
label_encoder = LabelEncoder()

# Flatten the DataFrame to a 1D array, fit transform, then reshape back
all_labels = label_encoder.fit_transform(league_data.values.flatten())

# Reshape and split into input features and target feature
X_encoded = all_labels.reshape(league_data.shape)[:, :-1]  # Input features
y_encoded = all_labels.reshape(league_data.shape)[:, -1]   # Target feature

# Since label_encoder was fit on all data, we use its classes_ to get the right input_dim
input_dim = len(label_encoder.classes_)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)

# Define the model architecture (or make changes to it)
model = Sequential([
    Embedding(input_dim=input_dim, output_dim=50, input_length=X_encoded.shape[1]),
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    BatchNormalization(),  # Example of an added layer
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(input_dim, activation='softmax')
])

# Compile the model with new settings
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])

# Now fit the model
model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))

model = Sequential([
    Embedding(input_dim=input_dim, output_dim=50, input_length=X_encoded.shape[1]),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    BatchNormalization(),
    Dense(256, activation='relu'),
    Dropout(0.3),
    Dense(input_dim, activation='softmax')
])

optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])

model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, BatchNormalization
from tensorflow.keras.optimizers import Adam

# Load the dataset
data_path = 'LeagueTeamDataClean.xlsx'
league_data = pd.read_excel(data_path)

# Define a function to prepare data subsets based on character frequency
def prepare_data(data_subset):
    label_encoder = LabelEncoder()
    all_labels = label_encoder.fit_transform(data_subset.values.flatten())
    X_encoded = all_labels.reshape(data_subset.shape)[:, :-1]  # Input features
    y_encoded = all_labels.reshape(data_subset.shape)[:, -1]   # Target feature
    input_dim = len(label_encoder.classes_)  # Number of unique classes
    return X_encoded, y_encoded, input_dim

# Define and compile the model
def compile_model(input_dim, input_length):
    model = Sequential([
        Embedding(input_dim=input_dim, output_dim=50, input_length=input_length),
        Flatten(),
        Dense(256, activation='relu'),
        Dropout(0.5),
        BatchNormalization(),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(input_dim, activation='softmax')
    ])
    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])
    return model

# Fit the model on a data subset
def fit_model(data_subset):
    X_encoded, y_encoded, input_dim = prepare_data(data_subset)
    input_length = X_encoded.shape[1]  # Correct input_length based on actual data
    model = compile_model(input_dim, input_length)
    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)
    history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))
    return history, model, X_test, y_test

# Filter the data based on character frequency
character_frequencies = {col: league_data[col].value_counts() for col in league_data.columns}
easy_cases = league_data[league_data.apply(lambda row: all(character_frequencies[col][row[col]] > 500 for col in league_data.columns), axis=1)]
average_cases = league_data[league_data.apply(lambda row: all(100 <= character_frequencies[col][row[col]] <= 500 for col in league_data.columns), axis=1)]
challenging_cases = league_data[league_data.apply(lambda row: all(character_frequencies[col][row[col]] < 100 for col in league_data.columns), axis=1)]

# Run the experiments
history_easy, model_easy, X_test_easy, y_test_easy = fit_model(easy_cases)
history_avg, model_avg, X_test_avg, y_test_avg = fit_model(average_cases)
history_chal, model_chal, X_test_chal, y_test_chal = fit_model(challenging_cases)

# Evaluate and plot the results
def plot_results(history, title):
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(title + ' Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(title + ' Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

plot_results(history_easy, 'Easy Case')
plot_results(history_avg, 'Average Case')
plot_results(history_chal, 'Challenging Case')